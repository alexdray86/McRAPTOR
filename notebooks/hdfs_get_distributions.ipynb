{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Distributions of Delay Times per trip and station\n",
    "\n",
    "The goal of this chapter is to create a distribution of arrival delays for each station / trip_id pair, to be used later on to compute transfer probabilities. These are then used in McRaptor implementation, to choose the best trip according to their time but also their __probability of success__.\n",
    "\n",
    "<div class='alert alert-info'><b>Any application without a proper name would be promptly killed.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'lgptguys_final'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8983</td><td>application_1589299642358_3520</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3520/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3520_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9081</td><td>application_1589299642358_3641</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3641/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3641_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9098</td><td>application_1589299642358_3660</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3660/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3660_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9112</td><td>application_1589299642358_3675</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3675/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3675_01_000005/ebouille\">Link</a></td><td></td></tr><tr><td>9121</td><td>application_1589299642358_3684</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3684/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3684_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9130</td><td>application_1589299642358_3694</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3694/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3694_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9145</td><td>application_1589299642358_3710</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3710/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3710_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9152</td><td>application_1589299642358_3716</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3716/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3716_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9153</td><td>application_1589299642358_3717</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3717/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3717_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9156</td><td>application_1589299642358_3721</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3721/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3721_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9157</td><td>application_1589299642358_3722</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3722/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3722_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9159</td><td>application_1589299642358_3724</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3724/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3724_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9160</td><td>application_1589299642358_3725</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3725/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3725_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9163</td><td>application_1589299642358_3729</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3729/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3729_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9164</td><td>application_1589299642358_3730</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3730/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3730_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9167</td><td>application_1589299642358_3733</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3733/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3733_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9168</td><td>application_1589299642358_3734</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3734/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3734_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9170</td><td>application_1589299642358_3737</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3737/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3737_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9171</td><td>application_1589299642358_3738</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3738/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3738_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9173</td><td>application_1589299642358_3740</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3740/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3740_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9174</td><td>application_1589299642358_3741</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3741/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3741_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9176</td><td>application_1589299642358_3743</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3743/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3743_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9177</td><td>application_1589299642358_3744</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3744/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3744_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9179</td><td>application_1589299642358_3746</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3746/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3746_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9180</td><td>application_1589299642358_3747</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3747/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3747_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9181</td><td>application_1589299642358_3748</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3748/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3748_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9182</td><td>application_1589299642358_3749</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3749/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3749_01_000001/ebouille\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\"conf\": {\n",
    "    \"spark.app.name\": \"lgptguys_final\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9183</td><td>application_1589299642358_3750</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3750/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3750_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "unknown magic command '%spark'\n",
      "UnknownMagic: unknown magic command '%spark'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Variable named username not found.\n"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import useful libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from geopy.distance import great_circle\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TimeTable data for routes / trips "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407\n",
      "+---------------+\n",
      "|stop_id_general|\n",
      "+---------------+\n",
      "|        8503078|\n",
      "|        8503088|\n",
      "|        8589111|\n",
      "|        8503376|\n",
      "|        8591190|\n",
      "+---------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Load data with stop_id of interest\n",
    "stop_times = spark.read.csv('data/lgpt_guys/stop_times_final_cyril.csv', header = True)\n",
    "stops_15km = stop_times.select(col('stop_id_general')).dropDuplicates()\n",
    "\n",
    "# print unique number of stop_id and show\n",
    "print stops_15km.count()\n",
    "stops_15km.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the [SBB actual data](https://opentransportdata.swiss/en/dataset/istdaten) in ORC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbb = spark.read.orc('/data/sbb/orc/istdaten')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset SBB data\n",
    "\n",
    "This notebook was ran twice to get two different set of distributions : \n",
    "- On one hand, delay distribution calculated only with delays having prognose status `geschaetz` or `real`. These are the distributions we use in priority whenever we have enough data in it for a given transfer.\n",
    "- On the other hand, delay distribution calculated with all delays from sbb, including status `prognose`, which means the delay is estimated. This is expected to be less precise, but whenever we have not enough data in `geschaetz` or `real`, this is still better than estimating ourself the delay.\n",
    "\n",
    "First step is to subset sbb dataset for `real/geschaetz` or `all` prognose_status and to write results in intermediate tables. \n",
    "\n",
    "__Delays with geschaetz/real status__\n",
    "\n",
    "We take only stop_id in 15 km range from Zurich HB using `stop_id_general` field from _stops_15km_ file. Then we filter only `an_prognose_status` and `ab_prognose_status` set to `geschaetz` or `real`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10848628\n",
      "+----------------+----------------+----------------+-------------------+-------------------+-------+\n",
      "|fahrt_bezeichner|ankunftszeit    |abfahrtszeit    |an_prognose        |ab_prognose        |stop_id|\n",
      "+----------------+----------------+----------------+-------------------+-------------------+-------+\n",
      "|85:11:10:002    |03.09.2018 21:51|                |03.09.2018 21:53:40|                   |8503000|\n",
      "|85:11:11:001    |                |03.09.2018 06:09|                   |03.09.2018 06:10:22|8503000|\n",
      "|85:11:12:001    |03.09.2018 10:51|                |03.09.2018 10:51:28|                   |8503000|\n",
      "+----------------+----------------+----------------+-------------------+-------------------+-------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "# Used to subset sbb table based on stop_id from stops_15km\n",
    "stop_id  = stops_15km.select('stop_id_general').collect()\n",
    "stop_idx = [item.stop_id_general for item in stop_id]\n",
    "\n",
    "# Make the subset dataframe\n",
    "sbb_filt = sbb.filter( ( sbb['bpuic'].isin(stop_idx) ) &\\\n",
    "                       ((sbb.an_prognose_status == 'REAL') | \\\n",
    "                        (sbb.an_prognose_status == 'GESCHAETZ') | \\\n",
    "                        (sbb.ab_prognose_status == 'REAL') | \\\n",
    "                        (sbb.ab_prognose_status == 'GESCHAETZ') ) ) \\\n",
    "              .select('fahrt_bezeichner', 'ankunftszeit', 'abfahrtszeit', \\\n",
    "                      'an_prognose', 'ab_prognose', \\\n",
    "                      col('bpuic').alias('stop_id'))\n",
    "\n",
    "print sbb_filt.count()\n",
    "sbb_filt.show(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write subset table in HDFS for better performance during later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save\n",
    "sbb_filt.write.format(\"orc\").save(\"data/lgpt_guys/sbb_filt_forDelays_GeschaetzAndReal.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Delay including prognose status__\n",
    "\n",
    "We take only stop_id in 15 km range from Zurich HB using `stop_id_general` field from _stops_15km_ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209398081\n",
      "+----------------+----------------+----------------+-------------------+-------------------+-------+\n",
      "|fahrt_bezeichner|ankunftszeit    |abfahrtszeit    |an_prognose        |ab_prognose        |stop_id|\n",
      "+----------------+----------------+----------------+-------------------+-------------------+-------+\n",
      "|85:11:10:002    |03.09.2018 21:51|                |03.09.2018 21:53:40|                   |8503000|\n",
      "|85:11:11:001    |                |03.09.2018 06:09|                   |03.09.2018 06:10:22|8503000|\n",
      "|85:11:12:001    |03.09.2018 10:51|                |03.09.2018 10:51:28|                   |8503000|\n",
      "+----------------+----------------+----------------+-------------------+-------------------+-------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "# Used to subset sbb table based on stop_id from stops_15km\n",
    "stop_id  = stops_15km.select('stop_id_general').collect()\n",
    "stop_idx = [item.stop_id_general for item in stop_id]\n",
    "\n",
    "# Make the subset dataframe\n",
    "sbb_filt_all = sbb.filter( sbb['bpuic'].isin(stop_idx) )\\\n",
    "                  .select('fahrt_bezeichner', 'ankunftszeit', 'abfahrtszeit', \\\n",
    "                          'an_prognose', 'ab_prognose', \\\n",
    "                          col('bpuic').alias('stop_id'))\n",
    "\n",
    "print sbb_filt_all.count()\n",
    "sbb_filt_all.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save\n",
    "sbb_filt_all.write.format(\"orc\").save(\"data/lgpt_guys/sbb_filt_forDelays_AllDelays_2.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of tables writen in `data/lgpt_guys/` :\n",
    "- sbb_filt_forDelays_GeschaetzAndReal.orc : Use geschaetz and real, < 15km, final stops from cyril data\n",
    "- sbb_filt_forDelays_AllDelays_2.orc : stops < 15km, final stops from cyril data\n",
    "- sbb_filt_forDelays_AllDelays.orc : MISTAKE done , do NOT use this one !!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work from translation tables \n",
    "\n",
    "We will use data generated in `match_datasets.ipynb`, that matches trip_id between _timetable_ and _sbb_ dataset. We begin by looking at all trip_id that are found in both dataset with at least 5 stations in common.\n",
    "\n",
    "Our goal is to find a match in sbb dataset for all _timetable_ trips (and not the other way around). So we will focus on getting this assymetrical correspondance table. \n",
    "\n",
    "In order to do that, we need to do multiple join, as we want to join 3 tables : _sbb_ data which contains information about delays, `joined_trip_atL5_3` table which contains translation between trip_id in two datasets, and `stop_time` which contains all the unique stop_id x trip_id used for later steps.\n",
    "- First, we join _sbb_ data `sbb_filt_forDelays_GeschaetzAndReal_2` with translation table `joined_trip_atL5_3` to get sbb data with information about _timetable_ trip_id. \n",
    "- We can then use this _timetable_ trip_id to join this first table with `stop_time` table, using a _left_outer_ join, so that we get an idea of how many matches are found overall.\n",
    "\n",
    "First we load SBB data. Following cells were ran twice : once for `geschaetz` / `real` delays only, and once for `all` delays. \n",
    "- `geschaetz` / `real` : load and use `/user/{}/sbb_filt_forDelays_GeschaetzAndReal_2.orc` table\n",
    "- `all` : load and use `/user/{}/sbb_filt_forDelays_AllDelays.orc` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10848628\n",
      "+----------------+----------------+----------------+-------------------+-------------------+-------+\n",
      "|fahrt_bezeichner|    ankunftszeit|    abfahrtszeit|        an_prognose|        ab_prognose|stop_id|\n",
      "+----------------+----------------+----------------+-------------------+-------------------+-------+\n",
      "|    85:11:10:002|12.10.2018 21:51|                |12.10.2018 21:51:50|                   |8503000|\n",
      "| 85:11:10293:004|                |13.10.2018 00:25|                   |13.10.2018 00:26:08|8503000|\n",
      "| 85:11:10293:004|13.10.2018 00:34|13.10.2018 00:35|13.10.2018 00:35:27|13.10.2018 00:36:44|8503016|\n",
      "+----------------+----------------+----------------+-------------------+-------------------+-------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "# Choose one table to work with \n",
    "#table_delays = 'sbb_filt_forDelays_AllDelays_2'\n",
    "table_delays = 'sbb_filt_forDelays_GeschaetzAndReal'\n",
    "\n",
    "# Load sbb data for a given table\n",
    "sbb_filt = spark.read.orc(\"data/lgpt_guys/{}.orc\".format(table_delays))\n",
    "\n",
    "# Print line count and show\n",
    "print(sbb_filt.count())\n",
    "sbb_filt.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the translation table we made in `match_datasets` notebook. This give a table with matching trip_id between _timetable_ and _sbb_ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921476\n",
      "+--------------------------+----------------------+-----+\n",
      "|trip_id                   |fahrt_bezeichner      |count|\n",
      "+--------------------------+----------------------+-----+\n",
      "|89.TA.26-721-j19-1.3.H    |85:773:778860-04720-1 |7    |\n",
      "|217.TA.1-17-A-j19-1.17.H  |85:31:987:000         |12   |\n",
      "|1890.TA.26-11-A-j19-1.27.R|85:3849:137108-21011-1|25   |\n",
      "|1612.TA.26-10-j19-1.11.R  |85:3849:617087-24010-1|19   |\n",
      "|113.TA.26-131-j19-1.6.R   |85:807:473534-31131-1 |3    |\n",
      "+--------------------------+----------------------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "translation_tab = spark.read.csv('data/lgpt_guys/match_datasets_translation.csv', header = True)\n",
    "\n",
    "# Print line counts and show\n",
    "print translation_tab.count()\n",
    "translation_tab.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first join sbb data `sbb_filt` with the translation table `translation_tab` to get trip_id in _timetable_ format on _sbb_ table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14816224\n",
      "+----------------+----------------+------------+-------------------+-----------+-------+-------+-----+\n",
      "|fahrt_bezeichner|ankunftszeit    |abfahrtszeit|an_prognose        |ab_prognose|stop_id|trip_id|count|\n",
      "+----------------+----------------+------------+-------------------+-----------+-------+-------+-----+\n",
      "|85:11:10173:004 |30.06.2019 16:04|            |30.06.2019 16:17:34|           |8503000|null   |null |\n",
      "|85:11:10173:004 |12.05.2019 16:04|            |12.05.2019 16:03:55|           |8503000|null   |null |\n",
      "|85:11:10213:004 |26.05.2019 16:30|            |26.05.2019 16:31:03|           |8503000|null   |null |\n",
      "|85:11:10213:004 |18.05.2019 16:30|            |18.05.2019 16:30:56|           |8503000|null   |null |\n",
      "|85:11:10213:004 |25.05.2019 16:30|            |25.05.2019 16:34:28|           |8503000|null   |null |\n",
      "+----------------+----------------+------------+-------------------+-----------+-------+-------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "joined_sbb = sbb_filt.join(translation_tab, on = ['fahrt_bezeichner'], how = 'left_outer')\n",
    "\n",
    "print joined_sbb.count()\n",
    "joined_sbb.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference table we use is `stop_times`. We load it and use it as a reference in a join against sbb table which now contains trip_id in _timetable_ format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260459\n",
      "+-------+----------------------+\n",
      "|stop_id|trip_id               |\n",
      "+-------+----------------------+\n",
      "|8591371|742.TA.26-46-j19-1.8.R|\n",
      "|8591358|742.TA.26-46-j19-1.8.R|\n",
      "|8591158|742.TA.26-46-j19-1.8.R|\n",
      "+-------+----------------------+\n",
      "only showing top 3 rows"
     ]
    }
   ],
   "source": [
    "# load ref table stop_times\n",
    "stop_times = spark.read.csv('data/lgpt_guys/stop_times_final_cyril.csv', header = True)\n",
    "\n",
    "# rename trip_id column\n",
    "stop_times = stop_times.select(stop_times.stop_id_general.alias('stop_id'), 'trip_id')\n",
    "\n",
    "# print line count and show \n",
    "print stop_times.count()\n",
    "stop_times.show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make the join between our reference table `stop_time` and previous join containing sbb data (used for delay computation). We can join them on `stop_id` and `trip_id` column, which in both case corresponds to _timetable_ trip_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13084796\n",
      "+----------------------+-------+-----+----------------+------------+------------+-----------+-----------+\n",
      "|trip_id               |stop_id|count|fahrt_bezeichner|ankunftszeit|abfahrtszeit|an_prognose|ab_prognose|\n",
      "+----------------------+-------+-----+----------------+------------+------------+-----------+-----------+\n",
      "|1.TA.26-163-j19-1.1.R |8590688|null |null            |null        |null        |null       |null       |\n",
      "|1.TA.26-89-j19-1.1.R  |8591209|null |null            |null        |null        |null       |null       |\n",
      "|10.TA.1-305-j19-1.1.R |8587018|null |null            |null        |null        |null       |null       |\n",
      "|10.TA.26-69-j19-1.2.H |8591122|null |null            |null        |null        |null       |null       |\n",
      "|10.TA.26-845-j19-1.2.H|8580879|null |null            |null        |null        |null       |null       |\n",
      "+----------------------+-------+-----+----------------+------------+------------+-----------+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Do the \n",
    "stop_times_join = stop_times.join(joined_sbb, on=['trip_id', 'stop_id'], \n",
    "                                  how='left_outer')\\\n",
    "                            .select('trip_id', 'stop_id', 'count',\n",
    "                                    'fahrt_bezeichner', 'ankunftszeit', 'abfahrtszeit',\n",
    "                                    'an_prognose', 'ab_prognose')\n",
    "\n",
    "print stop_times_join.count()\n",
    "stop_times_join.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute arrival delays using the following approach : \n",
    "- arrival_true ( = `an_prognose`) - arrival_expected ( = `ankunftszeit`). Train being late have a positive delay and trains being ahead of schedule a negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+-------------------+-------------+-------------+-------+\n",
      "|stop_id|trip_id               |arrival_true       |DiffInSeconds|DiffInMinutes|weekday|\n",
      "+-------+----------------------+-------------------+-------------+-------------+-------+\n",
      "|8503003|176.TA.26-3-j19-1.12.H|2019-08-01 01:51:47|47           |0            |Thu    |\n",
      "|8503000|176.TA.26-3-j19-1.12.H|2019-08-01 01:54:56|-4           |0            |Thu    |\n",
      "|8503020|176.TA.26-3-j19-1.12.H|2019-08-01 01:59:18|18           |0            |Thu    |\n",
      "|8503003|176.TA.26-3-j19-1.12.H|2019-06-10 01:52:23|83           |1            |Mon    |\n",
      "|8503000|176.TA.26-3-j19-1.12.H|2019-06-10 01:55:17|17           |0            |Mon    |\n",
      "|8503020|176.TA.26-3-j19-1.12.H|2019-06-10 01:59:16|16           |0            |Mon    |\n",
      "|8503003|176.TA.26-3-j19-1.12.H|2019-05-30 01:52:03|63           |1            |Thu    |\n",
      "|8503000|176.TA.26-3-j19-1.12.H|2019-05-30 01:54:58|-2           |0            |Thu    |\n",
      "|8503020|176.TA.26-3-j19-1.12.H|2019-05-30 01:59:17|17           |0            |Thu    |\n",
      "|8503003|176.TA.26-3-j19-1.12.H|2019-04-22 01:52:36|96           |1            |Mon    |\n",
      "+-------+----------------------+-------------------+-------------+-------------+-------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "stop_times_diff = stop_times_join.select( col(\"an_prognose\").alias(\"arrival_true\"),\\\n",
    "                              col(\"ankunftszeit\").alias(\"arrival_expected\"),\\\n",
    "                              'trip_id', 'stop_id')\\\n",
    "              .withColumn('arrival_true',to_timestamp(col('arrival_true'),\\\n",
    "                                                          format='dd.MM.yyyy HH:mm:ss'))\\\n",
    "              .withColumn('arrival_expected',to_timestamp(col('arrival_expected'),\\\n",
    "                                                           format='dd.MM.yyyy HH:mm'))\\\n",
    "              .withColumn('DiffInSeconds',col('arrival_true').cast(LongType()) - col('arrival_expected').cast(LongType()))\\\n",
    "              .withColumn('DiffInMinutes',(col('DiffInSeconds')/60).cast('integer'))\\\n",
    "              .select(\"stop_id\", \"trip_id\", \"arrival_true\", \"DiffInSeconds\", \"DiffInMinutes\",\\\n",
    "                        date_format('arrival_expected', 'E').alias('weekday'))\n",
    "\n",
    "# Remove Saturday and Sunday weekdays from table - show\n",
    "stop_times_diff = stop_times_diff.filter( (stop_times_diff.weekday != \"Sun\") & (stop_times_diff.weekday != \"Sat\") )\n",
    "stop_times_diff.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the difference between expected arrival time `ankunftszeit` and the actual arrival time `an_prognose` to compute the delay. This delay in seconds `DiffInSeconds` is then converted to a delay in minutes `DiffInMinutes` and converted to integer type. \n",
    "\n",
    "This `DiffInMinutes` is used in next step to do a pivot on the table, in order to get one column per unique value of `DiffInMinutes`. Before being able to do that, we bound the values contained in `DiffInMinutes` in the range [-1, +30] :\n",
    "- minimum 'delay' is -1 : it contains all arrivals ahead of schedule.\n",
    "- maximum delay is +30, it contains all delays $\\geq +30$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|stop_id|trip_id                |-1 |0  |1  |2  |3  |4  |5  |6  |7  |8  |9  |10 |11 |12 |13 |14 |15 |16 |17 |18 |19 |20 |21 |22 |23 |24 |25 |26 |27 |28 |29 |30 |\n",
      "+-------+-----------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|8503020|45.TA.26-7-A-j19-1.12.H|0  |596|148|53 |17 |4  |5  |4  |2  |1  |1  |0  |1  |1  |0  |1  |1  |0  |0  |1  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |\n",
      "|8503128|179.TA.26-14-j19-1.37.R|0  |523|215|57 |27 |5  |5  |0  |0  |1  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |1  |0  |0  |0  |0  |0  |0  |0  |\n",
      "|8594307|44.TA.1-11-B-j19-1.2.H |0  |50 |126|42 |9  |4  |1  |1  |1  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |\n",
      "|8502221|54.TA.26-5-A-j19-1.28.R|152|492|116|41 |14 |6  |8  |2  |1  |0  |0  |1  |0  |1  |2  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |\n",
      "|8503000|390.TA.26-2-j19-1.165.H|2  |516|224|50 |16 |10 |7  |2  |5  |3  |0  |0  |1  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |\n",
      "+-------+-----------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# we bound distribution to this \n",
    "lower_bound = -1\n",
    "upped_bound = +30\n",
    "\n",
    "# Bound minute delays between lower_bound and upper_bound \n",
    "stop_times_bounded = stop_times_diff.withColumn('DiffInMinutes_bounded1',\\\n",
    "                                        greatest(col('DiffInMinutes'), lit(lower_bound) ))\\\n",
    "                                    .withColumn('DiffInMinutes_bounded2',\\\n",
    "                                        least(col('DiffInMinutes_bounded1'), lit(upped_bound) ))\n",
    "\n",
    "# Pivot table using delay rounded to minute, fill null with 0\n",
    "stop_times_distribution = stop_times_bounded.groupBy('stop_id', 'trip_id')\\\n",
    "                                            .pivot(\"DiffInMinutes_bounded2\").count()\\\n",
    "                                            .na.fill(0)\n",
    "\n",
    "stop_times_distribution.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last move is to compute a unique key per line, corresponding to `trip_id` x `stop_id`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|key                             |-1 |0  |1  |2  |3  |4  |5  |6  |7  |8  |9  |10 |11 |12 |13 |14 |15 |16 |17 |18 |19 |20 |21 |22 |23 |24 |25 |26 |27 |28 |29 |30 |\n",
      "+--------------------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|1.TA.26-20-j19-1.1.R__8503000   |0  |25 |78 |18 |5  |2  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |\n",
      "|1.TA.26-20-j19-1.1.R__8503003   |0  |83 |31 |9  |4  |1  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |\n",
      "|1.TA.26-20-j19-1.1.R__8503101   |3  |89 |22 |6  |5  |1  |0  |1  |1  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |\n",
      "|1.TA.26-20-j19-1.1.R__8503104   |0  |83 |21 |13 |4  |3  |2  |0  |1  |0  |1  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |\n",
      "|1.TA.80-173-Y-j19-1.1.H__8502276|0  |315|61 |17 |2  |1  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |1  |0  |0  |0  |0  |0  |0  |0  |0  |0  |0  |\n",
      "+--------------------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "stop_times_distrib_wKey = stop_times_distribution.orderBy('trip_id', 'stop_id')\\\n",
    "                                       .withColumn('key2', concat(col('trip_id'), lit('__'), col('stop_id')))\\\n",
    "                                       .drop('trip_id').drop('stop_id')\\\n",
    "                                       .select(col('key2').alias('key'), \"*\")\\\n",
    "                                       .drop('key2')\n",
    "\n",
    "stop_times_distrib_wKey.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the number of lines we have related to the total number of unique `trip_id` x `stop_id`, we can compare it to the ref stop_time table, where each line correspond to a unique `trip_id` x `stop_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reference table stop_times number of lines : 260459\n",
      "distribution table number of unique keys   : 16760"
     ]
    }
   ],
   "source": [
    "print \"reference table stop_times number of lines : {}\".format(stop_times.count())\n",
    "print \"distribution table number of unique keys   : {}\".format(stop_times_distrib_wKey.select(\"key\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write tables on hdfs. Differente path depending on which table we are working with (`geschaetz`/`real` or `all`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "which_distribution = 'geschaetzAndReal'\n",
    "#which_distribution = 'allDelays' \n",
    "\n",
    "stop_times_distrib_wKey.write.csv('data/lgpt_guys/distribution_{}_6.csv'.format(which_distribution), \\\n",
    "                                  header = True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also write it on /user/ folder to be able to load it in local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "username = 'acoudray'\n",
    "stop_times_distrib_wKey.write.csv('/user/{0}/distribution_{1}_6.csv'.format(username, which_distribution), \\\n",
    "                                  header = True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note : Last tables written_\n",
    "- data/lgpt_guys/distribution_allDelays_6.csv : contains distribution delays from all SBB data including arrival time with `prognose` status. Made with FULL sbb dataset. Translation table with 2 stop - time in the overlap only \n",
    "- data/lgpt_guys/distribution_allDelays_5.csv : contains distribution delays from all SBB data including arrival time with `prognose` status. Made with FULL sbb dataset.\n",
    "- data/lgpt_guys/distribution_geschaetzAndReal_5.csv : contains distribution delays for arrival time with status `geschaetz` or `real` only. Made with FULL sbb dataset.\n",
    "- data/lgpt_guys/distribution_allDelays_4.csv : contains distribution delays from all SBB data including arrival time with `prognose` status. Made with 13-17 May sbb dataset.\n",
    "- data/lgpt_guys/distribution_geschaetzAndReal_4.csv : contains distribution delays for arrival time with status `geschaetz` or `real` only. Made with 13-17 May sbb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use local python to make dictionnary on local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1.TA.26-20-j19-1.1.R__8503000',\n",
       "  array([ 0, 25, 78, 18,  5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])),\n",
       " ('1.TA.26-20-j19-1.1.R__8503003',\n",
       "  array([ 0, 83, 31,  9,  4,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])),\n",
       " ('1.TA.26-20-j19-1.1.R__8503101',\n",
       "  array([ 3, 89, 22,  6,  5,  1,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])),\n",
       " ('1.TA.26-20-j19-1.1.R__8503104',\n",
       "  array([ 0, 83, 21, 13,  4,  3,  2,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])),\n",
       " ('1.TA.80-173-Y-j19-1.1.H__8502276',\n",
       "  array([  0, 315,  61,  17,   2,   1,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0])),\n",
       " ('1.TA.80-173-Y-j19-1.1.H__8502277',\n",
       "  array([ 29, 325,  27,   5,   1,   1,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0])),\n",
       " ('1.TA.80-173-Y-j19-1.1.H__8503508',\n",
       "  array([  0,   3,   1,  67, 184,  90,  30,  10,   1,   3,   1,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0])),\n",
       " ('1.TA.80-177-Y-j19-1.1.H__8502273',\n",
       "  array([ 28, 165, 106,  43,  16,  16,   2,   1,   1,   1,   0,   0,   0,\n",
       "           1,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0])),\n",
       " ('1.TA.80-177-Y-j19-1.1.H__8502276',\n",
       "  array([ 96, 259,  22,   4,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0])),\n",
       " ('1.TA.80-177-Y-j19-1.1.H__8502277',\n",
       "  array([  0, 280,  77,  22,   2,   2,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%local\n",
    "\n",
    "from hdfs3 import HDFileSystem\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pickle \n",
    "import gzip\n",
    "from itertools import islice\n",
    "\n",
    "hdfs = HDFileSystem(host='hdfs://iccluster044.iccluster.epfl.ch', port=8020, user='ebouille')\n",
    "\n",
    "username = 'acoudray'\n",
    "which_distribution = 'geschaetzAndReal'\n",
    "#which_distribution = 'allDelays'\n",
    "\n",
    "# Load distribution file from HDFS and concatenate individual csv\n",
    "distrib_files = hdfs.glob('/user/{0}/distribution_{1}_6.csv/*.csv'.format(username, which_distribution))\n",
    "distrib = pd.DataFrame()\n",
    "for file in distrib_files:\n",
    "    with hdfs.open(file) as f:\n",
    "        distrib = distrib.append(pd.read_csv(f))\n",
    "distrib = distrib.set_index('key')\n",
    "\n",
    "# zip index and values to get {key : np.array()} shape \n",
    "d = dict(zip(distrib.index, np.array(distrib.values)))\n",
    "\n",
    "# Write it to local \n",
    "if which_distribution == 'allDelays':\n",
    "    with gzip.open(\"../data/d_all.pkl.gz\".format(which_distribution), \"wb\") as output_file:\n",
    "        pickle.dump(d, output_file)\n",
    "elif which_distribution == 'geschaetzAndReal':\n",
    "    with gzip.open(\"../data/d_real.pkl.gz\".format(which_distribution), \"wb\") as output_file:\n",
    "        pickle.dump(d, output_file)\n",
    "    \n",
    "# Functon to take a slice from a dictionnary - head equivalent\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "# display a slice of it\n",
    "take(10, d.items())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
